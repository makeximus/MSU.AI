{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLCq2gMc1ZQB"
      },
      "source": [
        "# Вспомогательный код"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV6GjHFT1ZQE"
      },
      "source": [
        "Установка и импорт необходимых библиотек:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "At2cnqqZ1ZQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fab94d29-3641-4b92-b76a-add33c22daf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/953.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/953.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m430.1/953.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m839.7/953.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0k3z6XP81ZQG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "\n",
        "from torch import nn\n",
        "from gym import Env, spaces\n",
        "from itertools import product\n",
        "from collections import deque\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwCSrXio1ZQH"
      },
      "source": [
        "Чтобы результаты воспроизводились, зафиксируем seeds:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8x6YK811ZQH"
      },
      "outputs": [],
      "source": [
        "def set_random_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "set_random_seed(42)\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKxmIVc11ZQI"
      },
      "source": [
        "# Задание 1. Написание среды для игры в крестики-нолики"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQfL-qlf1ZQI"
      },
      "source": [
        "В этом задании вы должны:\n",
        "\n",
        "- Реализовать собственную среду для игры в крестики нолики на основе `gymnasium.Env`, которая будет использоваться в других заданиях.\n",
        "- Протестировать работу среды на примере игры двух случайных агентов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRoDP_3v1ZQI"
      },
      "source": [
        "## Формат результата\n",
        "* Собственная работающая среда `TicTacToeEnv`.\n",
        "* Играющие случайные агенты.\n",
        "* Посчитанный винрейт для `'X'` для игры случайных агентов:\n",
        "\n",
        "    ```\n",
        "X wins in 58.53% games\n",
        "    ```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-_4kNMv1ZQI"
      },
      "source": [
        "## Среда для игры в крестики-нолики\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP0gj62l1ZQI"
      },
      "source": [
        "**Описание среды:**\n",
        "\n",
        "1. Игровое поле имеет размер 3×3, которое по ходу игры будет заполняться маркерами игроков `'X'` и `'0'`. В классе ниже отрисовка игрового поля реализована в методе `render`.\n",
        "\n",
        "    Пример игрового поля:\n",
        "\n",
        "    ```\n",
        "0|X|0\n",
        "_|X|0\n",
        "X|_|_\n",
        "    ```\n",
        "2. Состояние среды отображается вектором (списком) из 9 чисел, в котором 0 обозначает незанятую ячейку, 1 — ячейку, занятую `'X'`, и −1 — ячейку, занятую `'0'`. Состояние среды хранится в атрибуте класса `self.cells`.\n",
        "\n",
        "    Пример состояния среды для игрового поля выше:\n",
        "    ```\n",
        "[-1, 1, -1, 0, 1, -1, 1, 0, 0]\n",
        "    ```\n",
        "3. В атрибуте `self.player` хранится `X` или `0` — символ игрока, который сейчас ходит (меняется после каждого изменения среды).\n",
        "\n",
        "4. В метод `step` передается `action` — номер ячейки, которую игрок хочет изменить. Агент может поставить соответствующий маркер только в незанятую ячейку посредством передачи номера ячейки в среду.\n",
        "\n",
        "5. Игра заканчивается (`self.done = True`) в двух случаях: победа одного из игроков (проверяется методом `self.check_for_win`) или отсутствие пустых клеток на поле (проверяется методом `self.check_for_draw`).\n",
        "\n",
        "6. Награда должна предоставляться за победу `'X'` в размере $+1$ очка, за победу `'0'`, соответственно, $-1$. В случае ничьей и нетерминальных состояний награда равна $0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azIzsQuM1ZQJ"
      },
      "source": [
        "Заполните недостающие фрагменты кода `# Your code here`. Помните, что пространство `self.action_space`, заполняемое игроками, [дискретно](https://gymnasium.farama.org/api/spaces/fundamental/#discrete)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tutj_jso1ZQJ"
      },
      "outputs": [],
      "source": [
        "class TicTacToeEnv(Env):\n",
        "    def __init__(self):\n",
        "        # Define default variable\n",
        "        self.cells = [0 for i in range(9)]  # environment state\n",
        "        self.player = \"X\"  # current player (changes every step)\n",
        "        self.done = False  # is the game over\n",
        "        self.winner = None  # who is the winner\n",
        "\n",
        "        # Symbols for rendering\n",
        "        self.markers = {1: \"X\", 0: \"_\", -1: \"0\"}\n",
        "\n",
        "        # Space https://gymnasium.farama.org/api/spaces/fundamental\n",
        "        self.action_space = spaces.Discrete(9)  # Your code here\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Bring game to initial state, define default variables.\n",
        "        \"\"\"\n",
        "        # Your code here\n",
        "        self.cells = [0 for i in range(9)]\n",
        "        self.player = 'X'\n",
        "        self.done = False\n",
        "        self.winner = None\n",
        "\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"\n",
        "        Рrint game board.\n",
        "        \"\"\"\n",
        "        cells = [self.markers[x] for x in self.cells]\n",
        "\n",
        "        for j in range(0, 9, 3):\n",
        "            print(\"|\".join([cells[i] for i in range(j, j + 3)]))\n",
        "\n",
        "    def legal_actions(self):\n",
        "        \"\"\"\n",
        "        Check for actions available: check free cells\n",
        "        \"\"\"\n",
        "        return [ind for ind, value in enumerate(self.cells) if value == 0]\n",
        "\n",
        "    def check_for_win(self, cells):\n",
        "        \"\"\"\n",
        "        Check that there is any win combination on the board.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        cells: list\n",
        "            Environment state\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            bool\n",
        "            True if win, False in over cases\n",
        "        \"\"\"\n",
        "        # Your code here\n",
        "        if self.cells[0] == self.cells[1] == self.cells[2] != 0:\n",
        "            return True\n",
        "        if self.cells[3] == self.cells[4] == self.cells[5] != 0:\n",
        "            return True\n",
        "        if self.cells[6] == self.cells[7] == self.cells[8] != 0:\n",
        "            return True\n",
        "        if self.cells[0] == self.cells[3] == self.cells[6] != 0:\n",
        "            return True\n",
        "        if self.cells[1] == self.cells[4] == self.cells[7] != 0:\n",
        "            return True\n",
        "        if self.cells[2] == self.cells[5] == self.cells[8] != 0:\n",
        "            return True\n",
        "        if self.cells[0] == self.cells[4] == self.cells[8] != 0:\n",
        "            return True\n",
        "        if self.cells[2] == self.cells[4] == self.cells[6] != 0:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    def check_for_draw(self, cells):\n",
        "        \"\"\"\n",
        "        Checking that the board is completely filled out.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        cells: list\n",
        "            Environment state\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            bool\n",
        "            True if the board is completely filled out, False in over cases\n",
        "        \"\"\"\n",
        "        if 0 not in cells:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Player input process\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        action: int\n",
        "            number of cell for change\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        observation: list\n",
        "            New environment state\n",
        "        reward: int\n",
        "            Reward: 1 if win of 'X', -1 if win of '0', 0 in othrer cases\n",
        "        self.done: bool\n",
        "            Game over flag\n",
        "        self.player: 'X' or '0'\n",
        "            Player who takes the next step\n",
        "        \"\"\"\n",
        "        # Check that action is possible\n",
        "        assert self.action_space.contains(action), \"impossible action\"\n",
        "        # Check that cell is empty\n",
        "        assert (\n",
        "            action in self.legal_actions()\n",
        "        ), \"not legal action\"\n",
        "\n",
        "        # Fill self.cells[action] depends on on whose turn (self.player) it is\n",
        "        if self.player == \"X\":\n",
        "            tmp = 1\n",
        "        elif self.player == \"0\":\n",
        "            tmp = -1\n",
        "        self.cells[action] = tmp # Your code here\n",
        "\n",
        "        observation = self.cells # Your code here\n",
        "\n",
        "        # Check that there is any win combination on the board\n",
        "        self.done = self.check_for_win(self.cells)  # Your code here\n",
        "\n",
        "        if self.done and self.player == \"X\":\n",
        "            reward = 1\n",
        "            self.winner = \"X\"\n",
        "\n",
        "        elif self.done and self.player == \"0\":\n",
        "            reward = -1\n",
        "            self.winner = \"0\"\n",
        "\n",
        "        else:\n",
        "            # Checking that the board is completely filled out\n",
        "            self.done = self.check_for_draw(self.cells)\n",
        "            reward = 0\n",
        "            self.winner = None\n",
        "\n",
        "        # toggle players\n",
        "        if self.player == \"X\":\n",
        "            self.player = \"0\"\n",
        "        else:\n",
        "            self.player = \"X\"\n",
        "\n",
        "        return observation, reward, self.done, self.player\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yD49Z1U1ZQK"
      },
      "source": [
        "## Случайный агент"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXdfynJP1ZQK"
      },
      "source": [
        "Реализуйте случайного агента, который выбирает действие случайным образом из доступных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sadC1qaD1ZQK"
      },
      "outputs": [],
      "source": [
        "class RandomAgent:\n",
        "    def __init__(self, mark=\"X\"):\n",
        "        self.mark = mark\n",
        "\n",
        "    def get_action(self, env):\n",
        "        \"\"\"\n",
        "        Sample random LEGAL action from action space\n",
        "        (use env.legal_actions and random.choice)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        action: int\n",
        "            number of cell for change\n",
        "        \"\"\"\n",
        "        # Your code here\n",
        "        action = np.random.choice(env.legal_actions())\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Iz3ufn31ZQK"
      },
      "outputs": [],
      "source": [
        "ttt = TicTacToeEnv()\n",
        "\n",
        "x_agent = RandomAgent(\"X\")\n",
        "o_agent = RandomAgent(\"0\")\n",
        "\n",
        "rand_players = {\"X\": x_agent, \"0\": o_agent}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu4Dcc7Y1ZQL"
      },
      "source": [
        "Визуализируйте игру между двумя случайными агентами:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ci_rmle51ZQL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cc01a17-2253-4dae-bd4b-f018ad4a1579"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_|_|X\n",
            "_|_|_\n",
            "_|_|_\n",
            "\n",
            "\n",
            "_|_|X\n",
            "_|0|_\n",
            "_|_|_\n",
            "\n",
            "\n",
            "_|_|X\n",
            "_|0|_\n",
            "_|_|X\n",
            "\n",
            "\n",
            "_|_|X\n",
            "_|0|0\n",
            "_|_|X\n",
            "\n",
            "\n",
            "X|_|X\n",
            "_|0|0\n",
            "_|_|X\n",
            "\n",
            "\n",
            "X|_|X\n",
            "_|0|0\n",
            "0|_|X\n",
            "\n",
            "\n",
            "X|X|X\n",
            "_|0|0\n",
            "0|_|X\n",
            "\n",
            "\n",
            "X wins! Reward is 1\n"
          ]
        }
      ],
      "source": [
        "ttt.reset()\n",
        "\n",
        "while not ttt.done:\n",
        "    # which agent from `rand_players` is playing (use `ttt.player` info)\n",
        "    player = ttt.player # Your code here\n",
        "    # action of this agent\n",
        "    action = rand_players[player].get_action(ttt) # Your code here\n",
        "    # step\n",
        "    state, reward, done, player = ttt.step(action) # Your code here\n",
        "    ttt.render()\n",
        "    print(\"\\n\")\n",
        "print(f\"{ttt.winner} wins! Reward is {reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nkil1GsB1ZQL"
      },
      "source": [
        "## Винрейт для 'X' для случайных агентов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA3RCHp01ZQL"
      },
      "source": [
        "Давайте сделаем бейзлайн, с которым мы будем сравнивать результаты игры агентов, которых мы будем обучать. Для этого посчитаем, в каком проценте случаев `X` выигрывает на 100000 играх между случайными игроками, и дальше будем пробовать улучшить этот результат."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psb3A5Ie1ZQL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baa54e52-a030-4127-9225-15f9600c9d66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X wins in 58.39% games\n"
          ]
        }
      ],
      "source": [
        "wins = {\"X\": 0, \"0\": 0}\n",
        "\n",
        "for i in range(100_000):\n",
        "    ttt.reset()\n",
        "\n",
        "    while not ttt.done:\n",
        "        player = rand_players[ttt.player]\n",
        "        action = player.get_action(ttt)\n",
        "\n",
        "        state, reward, done, player = ttt.step(action)\n",
        "\n",
        "    if ttt.winner is not None:\n",
        "        wins[ttt.winner] += 1\n",
        "\n",
        "print(f'X wins in {round((wins[\"X\"]/100_000)*100, 2)}% games')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prgr0wwh1ZQL"
      },
      "source": [
        "# Задание 2. Обучение агента игре в крестики-нолики с помощью Q-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4j5kiFx1ZQL"
      },
      "source": [
        "Создайте агента для игры в крестики-нолики и обучите его с помощью Q-learning, протестируйте жадного и $\\varepsilon$-жадного агента."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMQrJlcG1ZQL"
      },
      "source": [
        "## Формат результата\n",
        "\n",
        "1. Обученные с помощью Q-learning агенты, игающие за `'X'`:\n",
        "- жадный,\n",
        "- $\\varepsilon$-жадный.\n",
        "2. Посчитанные для агентов винрейты при игре против случайного агента, играющего за `'0'`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha1DAu0O1ZQM"
      },
      "source": [
        "## Код Q-learning агента\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMyYIiqX1ZQM"
      },
      "source": [
        "В этой части задания вам необходимо реализовать Q-learning агента.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49q2Xc031ZQM"
      },
      "source": [
        "Вам необходимо будет заполнить `# Your code here`:\n",
        "- метод `set_states`: нужно выбрать из множества комбинаций −1, 0 и 1 возможные состояния среды перед ходом `‘X’` (`‘X’` всегда ходит первым, поэтому количество 1 в состоянии среды должно быть равно количеству −1) и перед ходом `’0’` (количество 1 в состоянии среды должно быть на один больше количества −1).\n",
        "- метод `set_Q_table`: нужно инициализировать случайными числами Q-значения всех легальных действий (легально заполнение пустых клеток) из этих состояний.\n",
        "- метод `get_action`: нужно реализовать выбор лучшего или случайного действия в зависимости от типа агента и значения `epsilon`.\n",
        "- метод `update_Q`: нужно реализовать основную формулу Q-learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0-OmOJE1ZQM"
      },
      "source": [
        "Основная формула Q-learning:\n",
        "\n",
        "$$Q(s,a) = Q(s,a)+α(R^a_{s} + \\gamma\\max_{a'}Q(s',a') -Q(s,a)),$$\n",
        "\n",
        "где $s$ — состояние среды `state` в начале хода агента, $a$ — действие `action` агента на данном ходе, $Q(s,a)$ — значение Q-table `self.Q[current_state][action]` для состояния $s$ и действия $a$, $R^a_{s}$ — награда `reward` за действие `a` из состояния `s`, $s'$ — состояние среды после хода игрока и его оппонента, $a'$ — следующее действие игрока, $α$ — скорость обучения, $\\gamma$ — дисконт за длинную игру."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCN13Ju91ZQM"
      },
      "source": [
        "**Совет:**\n",
        "- При обновлении Q-значений (метод `update_Q`) учтите, что терминальное состояние игры не присутствует в Q-таблице (из него уже нельзя делать ход) и, соответственно, для состояний, предшествующих ему, $\\max_{a'}Q(s',a')$ будет равно нулю.\n",
        "- При победе `'X'` выдается награда $+1$, а в случае победы `'0'` — награда $-1$. Агент, играющий `'X',` должен выбирать действие с максимальным Q-значением, а `'0'` — с минимальным Q-значением.\n",
        "- При обновлении Q-значений для действий из определенного состояния учтите, что $s'$ будет состоянием игрового поля не после хода игрока, а после хода оппонента."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJofilZu1ZQN"
      },
      "outputs": [],
      "source": [
        "from itertools import product\n",
        "\n",
        "\n",
        "class QTableAgent:\n",
        "    def __init__(\n",
        "            self, alpha=0.05, gamma=0.9, mark=\"X\", epsilon=0.,\n",
        "            epsilon_off=True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        alpha: float\n",
        "            learning rate\n",
        "        gamma: float\n",
        "            discount coefficient\n",
        "        mark: str\n",
        "            'X' or '0' - player symbol\n",
        "        epsilon: float\n",
        "            epsilon for epsilon-greedy agent\n",
        "        epsilon_off: boolean\n",
        "            if True -'greedy' learning strategy or inference,\n",
        "            if False 'epsilon-greedy' learning strategy\n",
        "        \"\"\"\n",
        "        self.mark = mark\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_off = epsilon_off\n",
        "\n",
        "        # Get possible for self.mark environment states\n",
        "        self.states = self.set_states()\n",
        "        # Init Q-table\n",
        "        self.Q = self.set_Q_table()\n",
        "\n",
        "    def set_states(self):\n",
        "        \"\"\"\n",
        "        Set possible for self.mark environment states\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        states: set\n",
        "            Set of possible for self.mark environment states\n",
        "        \"\"\"\n",
        "        # Set of all possible marker compositions\n",
        "        states = set(product(*[list(range(-1, 2)) for _ in range(9)]))\n",
        "        # Subset of states for X player\n",
        "        # contains states in which both players took equal number of actions\n",
        "        # (since X goes first)\n",
        "        if self.mark == \"X\": # select with condition\n",
        "            states = [i for i in states if i.count(-1) == i.count(1)] # Your code here\n",
        "\n",
        "        # Subset of states for 0 player\n",
        "        # contains states in which X player took one more action than 0 player\n",
        "        # (since 0 goes second)\n",
        "\n",
        "        elif self.mark == \"0\": # select with condition\n",
        "            states = [i for i in states if i.count(-1) == (i.count(1) + 1)] # Your code here\n",
        "        return states\n",
        "\n",
        "    def set_Q_table(self):\n",
        "        \"\"\"\n",
        "        Init Q-table.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Q: dict\n",
        "            Q-table[state][action] for possible states and actions with\n",
        "            random gauss (mean=0, sigma=0.1)\n",
        "        \"\"\"\n",
        "        Q = {}\n",
        "        # Match legal actions for each possible action in each state with\n",
        "        # random initial number\n",
        "        for state in self.states:\n",
        "            Q[state] = {}\n",
        "            # Your code here\n",
        "            for i in range(len(state)):\n",
        "                if state[i] == 0:\n",
        "                    Q[state][i] = np.random.rand() # Your code here\n",
        "        return Q\n",
        "\n",
        "    def get_action(self, env):\n",
        "        \"\"\"\n",
        "        Sample optimal or random action.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        env: TicTacToeEnv\n",
        "            environment\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        action: int\n",
        "            number of cell for change\n",
        "        \"\"\"\n",
        "        state = tuple(env.cells)\n",
        "        rand = random.uniform(0, 1)\n",
        "\n",
        "        if self.epsilon_off or rand >= self.epsilon:\n",
        "            # Sample optimal action (based on greediness)\n",
        "            if self.mark == \"X\":\n",
        "                action = max(set(self.Q[state].keys()) & set(env.legal_actions())) # Your code here\n",
        "            else:\n",
        "                action = min(set(self.Q[state].keys()) & set(env.legal_actions()))# Your code here\n",
        "        else:\n",
        "            # Sample random  action\n",
        "            action = np.random.choice(env.legal_actions())# Your code here\n",
        "\n",
        "        return action\n",
        "\n",
        "    def update_Q(self, current_state, action, next_state, reward, done):\n",
        "        \"\"\"\n",
        "        Q-table update.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        current_state: list\n",
        "            Current environment state\n",
        "        action: int\n",
        "            Current agent action\n",
        "        next_state: list\n",
        "            Environment state after agent action and opponent action\n",
        "        reward: int\n",
        "            Reward\n",
        "        done: bool\n",
        "            Game over flag\n",
        "        \"\"\"\n",
        "        current_state = tuple(current_state)\n",
        "        if not done:\n",
        "            next_state = tuple(next_state)\n",
        "            next_state_value = max(self.Q[next_state], key=self.Q[next_state].get)\n",
        "        else:\n",
        "            next_state_value = 0\n",
        "\n",
        "        # Q-learning update folmula\n",
        "        self.Q[current_state][action] += self.alpha * (reward + self.gamma * next_state_value - (self.Q[current_state][action]))# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3OySO2Z1ZQN"
      },
      "source": [
        "## Обучение жадного агента `'X'`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwhu5LCi1ZQN"
      },
      "source": [
        "Обучите агента, играющего за `X`, придерживающегося жадной стратегии, против **случайного** агента, играющего за `0`, на 1 миллионе игр и сравните их винрейты между собой и с бейзлайном."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G2Xs7yy1ZQN"
      },
      "source": [
        "**Совет:**\n",
        "- При обучении учтите, что $s'$ будет состоянием игрового поля не после хода игрока, а после хода оппонента.\n",
        "- Помните, что `list` в python является [изменяемым типом данных](https://realpython.com/python-mutable-vs-immutable-types/). Используйте `.copy()` чтобы изолировать состояние среды, подаваемое агенту."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygq0TYzT1ZQO"
      },
      "outputs": [],
      "source": [
        "ttt = TicTacToeEnv()\n",
        "\n",
        "x_agent = QTableAgent(0.05, 0.9, \"X\", epsilon=0.1, epsilon_off=True)\n",
        "\n",
        "o_agent = RandomAgent(\"0\")\n",
        "\n",
        "players = {\"X\": x_agent, \"0\": o_agent}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mo8VtWYD1ZQO"
      },
      "outputs": [],
      "source": [
        "for i in range(1_000_000):\n",
        "    ttt.reset()\n",
        "    while not ttt.done:\n",
        "        player = players[ttt.player]\n",
        "        if player.mark == \"X\":\n",
        "            action_x = player.get_action(ttt)  # Your code here\n",
        "            current_state = ttt.cells.copy() # Your code here\n",
        "            state, reward, done, _ = ttt.step(action_x) # Your code here\n",
        "            if done:\n",
        "\n",
        "                players[\"X\"].update_Q(current_state, action_x, None, reward, done) # Your code here\n",
        "        else:\n",
        "            action_o = player.get_action(ttt) # Your code here\n",
        "            next_state, reward, done, _ = ttt.step(action_o) # Your code here\n",
        "\n",
        "            players[\"X\"].update_Q(current_state, action_x, next_state, reward, done) # Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNn0ffht1ZQO"
      },
      "source": [
        "Посмотрим на винрейт:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmCDuDH01ZQO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4f13d4f-be9a-4a6d-d78d-1fb011a17594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X wins in 78.14% games\n"
          ]
        }
      ],
      "source": [
        "wins = {\"X\": 0, \"0\": 0}\n",
        "\n",
        "for i in range(100_000):\n",
        "    ttt.reset()\n",
        "\n",
        "    while not ttt.done:\n",
        "        player = players[ttt.player]\n",
        "        action = player.get_action(ttt)\n",
        "\n",
        "        state, reward, done, player = ttt.step(action)\n",
        "\n",
        "    if ttt.winner is not None:\n",
        "        wins[ttt.winner] += 1\n",
        "\n",
        "print(f'X wins in {round((wins[\"X\"]/100000)*100, 2)}% games')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8sAYVC91ZQO"
      },
      "source": [
        "## Обучение $\\varepsilon$-жадного агента `'X'`:\n",
        "\n",
        "Обучите агента, играющего за `X`, придерживающегося $\\varepsilon$-жадной стратегии, против **случайного** агента, играющего за `0`, на 1 миллионе игр и сравните их винрейты между собой и с бейзлайном."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjeZmzaG1ZQO"
      },
      "outputs": [],
      "source": [
        "ttt = TicTacToeEnv()\n",
        "\n",
        "x_agent = QTableAgent(0.05, 0.9, \"X\", epsilon=0.1, epsilon_off=False)\n",
        "o_agent = RandomAgent(\"0\")\n",
        "\n",
        "players = {\"X\": x_agent, \"0\": o_agent}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLTX3GUa1ZQP"
      },
      "outputs": [],
      "source": [
        "for i in range(1_000_000):\n",
        "    ttt.reset()\n",
        "    while not ttt.done:\n",
        "        player = players[ttt.player]\n",
        "        if player.mark == \"X\":\n",
        "            action_x = player.get_action(ttt) # Your code here\n",
        "            current_state = ttt.cells.copy() # Your code here\n",
        "            state, reward, done, _ = ttt.step(action_x) # Your code here\n",
        "            if done:\n",
        "                players[\"X\"].update_Q(current_state, action_x, None, reward, done) # Your code here\n",
        "        else:\n",
        "            action_o = player.get_action(ttt)# Your code here\n",
        "            next_state, reward, done, _ = ttt.step(action_o) # Your code here\n",
        "\n",
        "            players[\"X\"].update_Q(current_state, action_x, next_state, reward, done) # Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB6_0Xou1ZQP"
      },
      "source": [
        "Посмотрим на винрейт. Обратите внимание на то, что мы **выключаем примешивание случайных действий при тестировании модели**. Без этого качество модели будет сильно занижено!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eatUPoL_1ZQP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7618dd79-2b22-4835-ce29-26f74927a413"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X wins in 78.09% games\n"
          ]
        }
      ],
      "source": [
        "wins = {\"X\": 0, \"0\": 0}\n",
        "players[\"X\"].epsilon_off = True\n",
        "\n",
        "for i in range(100_000):\n",
        "    ttt.reset()\n",
        "\n",
        "    while not ttt.done:\n",
        "        player = players[ttt.player]\n",
        "        action = player.get_action(ttt)\n",
        "\n",
        "        state, reward, done, player = ttt.step(action)\n",
        "\n",
        "    if ttt.winner is not None:\n",
        "        wins[ttt.winner] += 1\n",
        "\n",
        "print(f'X wins in {round((wins[\"X\"]/100_000)*100, 2)}% games')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r3Clzi41ZQP"
      },
      "source": [
        "## Попробуйте сразиться с вашим обученным агентом\n",
        "\n",
        "Можете попробовать сразиться с вашим обученным агентом.\n",
        "В качестве ввода можете использовать либо ручной ввод, либо случайный:\n",
        "\n",
        "*   `human_action = int(input())  # manual input`\n",
        "*   `human_action = np.random.choice(ttt.legal_actions())`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xS6CxwA51ZQP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82ec341d-7820-4ee7-e38d-0fe5fe4aaeec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_|_|_\n",
            "_|_|_\n",
            "_|_|_\n",
            "\n",
            "\n",
            "It's X turn\n",
            "_|_|_\n",
            "_|_|_\n",
            "_|_|X\n",
            "\n",
            "\n",
            "_|_|_\n",
            "_|_|_\n",
            "_|_|X\n",
            "\n",
            "\n",
            "It's 0 turn\n",
            "chose action: [0, 1, 2, 3, 4, 5, 6, 7]\n",
            "4\n",
            "_|_|_\n",
            "_|0|_\n",
            "_|_|X\n",
            "\n",
            "\n",
            "_|_|_\n",
            "_|0|_\n",
            "_|_|X\n",
            "\n",
            "\n",
            "It's X turn\n",
            "_|_|_\n",
            "_|0|_\n",
            "_|X|X\n",
            "\n",
            "\n",
            "_|_|_\n",
            "_|0|_\n",
            "_|X|X\n",
            "\n",
            "\n",
            "It's 0 turn\n",
            "chose action: [0, 1, 2, 3, 5, 6]\n",
            "6\n",
            "_|_|_\n",
            "_|0|_\n",
            "0|X|X\n",
            "\n",
            "\n",
            "_|_|_\n",
            "_|0|_\n",
            "0|X|X\n",
            "\n",
            "\n",
            "It's X turn\n",
            "_|_|_\n",
            "_|0|X\n",
            "0|X|X\n",
            "\n",
            "\n",
            "_|_|_\n",
            "_|0|X\n",
            "0|X|X\n",
            "\n",
            "\n",
            "It's 0 turn\n",
            "chose action: [0, 1, 2, 3]\n",
            "2\n",
            "_|_|0\n",
            "_|0|X\n",
            "0|X|X\n",
            "\n",
            "\n",
            "0 wins! Reward is -1\n"
          ]
        }
      ],
      "source": [
        "ttt.reset()\n",
        "\n",
        "while not ttt.done:\n",
        "    ttt.render()\n",
        "    print(\"\\n\")\n",
        "    print(f\"It's {ttt.player} turn\")\n",
        "    if ttt.player == \"X\":\n",
        "        action = players[\"X\"].get_action(ttt)\n",
        "        state, reward, done, player = ttt.step(action)\n",
        "\n",
        "        ttt.render()\n",
        "        print(\"\\n\")\n",
        "        continue\n",
        "    else:\n",
        "        print(f\"chose action: {ttt.legal_actions()}\")\n",
        "        human_action = int(input())  # manual input\n",
        "        #human_action = np.random.choice(ttt.legal_actions())\n",
        "        state, reward, done, player = ttt.step(human_action)\n",
        "\n",
        "        ttt.render()\n",
        "        print(\"\\n\")\n",
        "\n",
        "print(f\"{ttt.winner} wins! Reward is {reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вывод: я победил!**\n",
        "При обучении жадного и эпсилон-жадного агента получили сопоставимое качество. Возможно, получится еще немного улучшить результат при подборе эпсилон."
      ],
      "metadata": {
        "id": "nPW20SA2XHY1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16WDWAYE1ZQQ"
      },
      "source": [
        "# Задание 3. Обучение агента игре в крестики-нолики при помощи DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoIC3MNH1ZQQ"
      },
      "source": [
        "Обучите с помощью Deep Q-learning агента игре в крестики-нолики."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APKG9jtH1ZQQ"
      },
      "source": [
        "## Формат результата\n",
        "Обученный с помощью Deep Q-learning агент, игающий за `'X'`, с винрейтом против случайного агента, играющего за `'0'`, ~85%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBbRRC6C1ZQQ"
      },
      "source": [
        "## Код DQN агента"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3Pxc9Ma1ZQQ"
      },
      "source": [
        "Для хранения experience replay будем использовать [deque](https://proproprogs.ru/structure_data/std-ochered-collectionsdeque-na-python)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycsfDy531ZQQ"
      },
      "source": [
        "В этой части задания вам необходимо реализовать DQN агента.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKlxjpyP1ZQQ"
      },
      "source": [
        "Вам необходимо будет заполнить `# Your code here`:\n",
        "- метод `set_Q_network`: нужно реализовать архитектуру сети. Архитектура сети должна быть устроена следующим образом: на вход принимаются 9 значений, которые соответствуют состоянию игрового поля, на выход выдаются 9 значений, которые соответствуют Q-значениям для 9 возможных действий, скрытые слои — на ваш выбор, поэкспериментируйте с разными архитектурами.\n",
        "- метод `get_action`: нужно реализовать выбор лучшего или случайного разрешенного действия в зависимости от типа агента и значения `epsilon`. Для получения списка разрешенных действий используйте `env.legal_actions()`.\n",
        "- метод `update_target_network`: необходимо копировать параметры сети `self.Q_net`(используется для обучения) в сеть `self.target_net` (используется для предсказания Q-значений для следующго состояния $s'$). Можно использовать `state_dict()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcjJqUOo1ZQQ"
      },
      "source": [
        "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/Exercises/EX15/dqn_loss.png\" alt=\"Drawing\" width=\"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZaoYPWK1ZQQ"
      },
      "source": [
        "**Совет:** обратите внимание, что не все 9 значений будут доступны для определенных состояний, учтите это в методе `get_action`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3sm6aL81ZQR"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "\n",
        "class DQNAgent(nn.Module):\n",
        "    def __init__(\n",
        "        self, gamma=0.9, mark=\"X\", memory_size=10000, epsilon=0.,\n",
        "        epsilon_off=True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        alpha: float\n",
        "            learning rate\n",
        "        gamma: float\n",
        "            discount coefficient\n",
        "        mark: str\n",
        "            'X' or '0' - player symbol\n",
        "        memory_size: int\n",
        "            size of memory buffer\n",
        "        epsilon: float\n",
        "            epsilon for epsilon-greedy agent\n",
        "        epsilon_off: boole\n",
        "            if True -'greedy' learning strategy or inference,\n",
        "            if False 'epsilon-greedy' learning strategy\n",
        "        \"\"\"\n",
        "        super(DQNAgent, self).__init__()\n",
        "        self.mark = mark\n",
        "        self.gamma = torch.tensor(gamma, dtype=float)\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_off = epsilon_off\n",
        "\n",
        "        # Experience replay\n",
        "        self.exp_replay = deque(maxlen=memory_size)\n",
        "        # Q-Network (for learning and Q(s, a))\n",
        "        self.Q_net = self.set_Q_network()\n",
        "        # Target-Network (for Q(s', a'))\n",
        "        self.target_net = self.set_Q_network()\n",
        "        self.update_target_network()\n",
        "\n",
        "    def set_Q_network(self):\n",
        "        \"\"\"Set Q_net architecture.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Q_net: nn.Sequential\n",
        "            Q_net architecture\n",
        "        \"\"\"\n",
        "        Q_net = nn.Sequential(\n",
        "            nn.Linear(9, 27),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(27, 81),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(81, 27),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(27, 9)) # Your code here\n",
        "        return Q_net\n",
        "\n",
        "    def forward(self, states):\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        states: list, np.array or torch.Tensor [batch_size, 9]\n",
        "            batch of environment states at the beginning of the agent's action\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Q_vals: torch.Tensor [batch_size, 9]\n",
        "            Q-vals for all 9 action (not all of this action legal)\n",
        "        \"\"\"\n",
        "        states = torch.Tensor(states)\n",
        "        Q_vals = self.Q_net(states)\n",
        "        return Q_vals\n",
        "\n",
        "    def get_action(self, Q_vals, env):\n",
        "        \"\"\"\n",
        "        Sample optimal or random legal action.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        Q_vals: torch.Tensor [batch_size, 9]\n",
        "            Q-vals for all 9 action (not all of this action legal)\n",
        "        env: TicTacToeEnv\n",
        "            environment\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        action: int\n",
        "            number of cell for change\n",
        "        \"\"\"\n",
        "        state = torch.Tensor(env.cells)\n",
        "        # Get legal action from env, transform to torch.int64 tensor\n",
        "\n",
        "        legal_actions = torch.LongTensor(env.legal_actions()) # Your code here\n",
        "\n",
        "        index = torch.zeros(9, dtype=bool)\n",
        "        index[legal_actions.to(torch.int64)] = True\n",
        "        rand = random.uniform(0, 1)\n",
        "        if self.epsilon_off or rand >= self.epsilon:\n",
        "            # Sample optimal action\n",
        "            if self.mark == \"X\":\n",
        "                best_q = max(torch.masked_select(Q_vals, index)) # Your code here\n",
        "            else:\n",
        "                best_q = min(torch.masked_select(Q_vals, index)) # Your code here\n",
        "            action = torch.logical_and(Q_vals == best_q, index).nonzero()[0].item()\n",
        "\n",
        "        else:\n",
        "            # Sample random action\n",
        "            action = np.random.choice(legal_actions) # Your code here\n",
        "\n",
        "        return int(action)\n",
        "\n",
        "    def add_to_memory(self, current_state, action, next_state, reward, done):\n",
        "        \"\"\"Add data to experience replay.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        current_state: torch.Tensor [batch_size, 9]\n",
        "            Current environment state\n",
        "        action: int\n",
        "            Current agent action\n",
        "        next_state: torch.Tensor [batch_size, 9]\n",
        "            Environment state after agent action and opponent action\n",
        "        reward: int\n",
        "            Reward\n",
        "        done: bool\n",
        "            Game over flag\n",
        "        \"\"\"\n",
        "        self.exp_replay.append((current_state, action, next_state, reward, done))\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"Use Q_net parameters to update target_net.\"\"\"\n",
        "        # Your code here\n",
        "        self.target_net.load_state_dict(self.Q_net.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGBXRcJr1ZQR"
      },
      "source": [
        "## Код обучения"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBi5LsxG1ZQR"
      },
      "source": [
        "Реализуйте функцию, рассчитывающую TD loss:\n",
        "$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
        "\n",
        "С Q-reference, определенным как:\n",
        "\n",
        "$$ \\large Q_{reference}(s,a) = R^a_{s} + \\gamma \\cdot \\max_{a'} Q_{target}(s', a'), $$\n",
        "\n",
        "где:\n",
        "* $R^a_{s}$ — награда `reward` за действие `a` из состояния `s`,\n",
        "* $Q_{target}(s',a')$ — $Q$-значение следующего состояния системы `next_states` и следующего действия, вычисленное `agent.target_net`,\n",
        "* $s, a, s'$ — текущее состояние `states`, действие `actions` и следующее состояние `next_states`,\n",
        "* $\\gamma$ — коэффициент дисконтирования `agent.gamma`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "894gfIgl1ZQR"
      },
      "source": [
        "**Совет:**\n",
        "- При расчете $Q_{reference}(s,a)$ `reference_q` учтите, что терминальное состояние игры не присутствует в Q-таблице (из него уже нельзя делать ход) и, соответственно, для состояний, предшествующих ему, $\\max_{a'} Q_{target}(s', a')$ будет равно нулю (используйте `is_not_done`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wk5EELGu1ZQS"
      },
      "outputs": [],
      "source": [
        "def compute_td_loss(batch, agent):\n",
        "    states = torch.tensor(np.array([x[0] for x in batch]), dtype=torch.float) # 9\n",
        "    actions = torch.tensor(np.array([x[1] for x in batch]), dtype=torch.int64) # 1\n",
        "    next_states = torch.tensor(np.array([x[2] for x in batch]), dtype=torch.float) # 9\n",
        "    rewards = torch.tensor(np.array([x[3] for x in batch]), dtype=torch.int64) # 1\n",
        "    is_not_done = 1 - torch.tensor(np.array([x[4] for x in batch]), dtype=torch.int64) # bool\n",
        "\n",
        "    q_vals = agent(states) # Your code here\n",
        "    current_q = q_vals[range(len(actions)), actions] # Your code here\n",
        "    target_q = agent.target_net(next_states)  # Your code here\n",
        "    reference_q = rewards + agent.gamma * target_q.max(axis=-1)[0] * is_not_done # Your code here\n",
        "    loss = torch.mean((current_q - reference_q.detach()) ** 2) # Your code here\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP4V45qK1ZQS"
      },
      "source": [
        "Реализуем функцию, выдающую винрейт игрока на 10к играх (не забываем выключать примешивание случайных действий при тестировании модели):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_R-ZJ9OI1ZQS"
      },
      "outputs": [],
      "source": [
        "def get_winrate():\n",
        "    wins = {\"X\": 0, \"0\": 0}\n",
        "    players[\"X\"].epsilon_off = True\n",
        "\n",
        "    for i in range(10_000):\n",
        "        ttt.reset()\n",
        "\n",
        "        while not ttt.done:\n",
        "            player = players[ttt.player]\n",
        "            if player.mark == \"0\":\n",
        "                action = player.get_action(ttt)\n",
        "            else:\n",
        "                q_vals = player(ttt.cells)\n",
        "                action = player.get_action(q_vals, ttt)\n",
        "\n",
        "            state, reward, done, player = ttt.step(action)\n",
        "\n",
        "        if ttt.winner is not None:\n",
        "            wins[ttt.winner] += 1\n",
        "    wr = (wins[\"X\"] / 10000) * 100\n",
        "    print(f\"X wins in {round(wr, 2)}% games\")\n",
        "    return wr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTXl4KH31ZQS"
      },
      "outputs": [],
      "source": [
        "target_wr = 85.0\n",
        "\n",
        "ttt = TicTacToeEnv()\n",
        "\n",
        "x_agent = DQNAgent(gamma=0.8, mark=\"X\", epsilon=0.2, epsilon_off=False)\n",
        "o_agent = RandomAgent(\"0\")\n",
        "\n",
        "players = {\"X\": x_agent, \"0\": o_agent}\n",
        "\n",
        "opt = torch.optim.Adam(players[\"X\"].Q_net.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjUh70251ZQS"
      },
      "source": [
        "Далее мы обучим игрока `X` против случайного игрока `0`. Задача — добиться винрейта ~85%.\n",
        "\n",
        "Процесс обучения строится следующим образом:\n",
        "* во время одного эпизода обучения на протяжении 50 игр собирается игровой опыт в буфер памяти агента, а именно текущее состояние, действие из него, следующее состояние, награда и флаг конца эпизода;\n",
        "* из памяти выбирается случайный батч и обновляются веса Q-сети;\n",
        "* после каждых 50 подобных эпизодов обновляются веса target network, следите за изменением винрейта после каждых 500 эпизодов.\n",
        "\n",
        "Вам необходимо заполнить пропуск `# Your code here`, в котором заполняется буфер памяти агента."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9ZfUHaU1ZQT"
      },
      "source": [
        "**Совет:**\n",
        "- При обучении учтите, что $s'$ будет состоянием игрового поля не после хода игрока, а после хода оппонента.\n",
        "- Обучение RL-модели неустойчиво. Небольшое изменение архитектуры сети или даже [выбор seed](https://www.alexirpan.com/2018/02/14/rl-hard.html) может существенно поменять результат."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HzjW1ZxJ6JK",
        "outputId": "cf7f915c-144b-48ae-ba3e-53c679dff854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([1, 0, 0, 0, -1, 0, -1, 0, 1], 2, [1, 0, 1, -1, -1, 1, -1, 0, 1], 0, False)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnyPtW1E1ZQT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d6645f-c38a-4c51-c706-ff68986d85d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X wins in 80.15% games\n",
            "X wins in 77.53% games\n",
            "X wins in 78.75% games\n",
            "X wins in 81.12% games\n",
            "X wins in 83.8% games\n",
            "X wins in 86.14% games\n"
          ]
        }
      ],
      "source": [
        "clear_output()\n",
        "\n",
        "for j in range(10_000):\n",
        "    for i in range(50):\n",
        "        # Your code here\n",
        "        ttt.reset()\n",
        "        while not ttt.done:\n",
        "\n",
        "            player = players[ttt.player]\n",
        "\n",
        "            if player.mark == 'X':\n",
        "                q_vals = player(ttt.cells.copy())\n",
        "                action_x = player.get_action(q_vals, ttt)\n",
        "                current_state = ttt.cells.copy()\n",
        "                state, reward, done, _ = ttt.step(action_x)\n",
        "\n",
        "                if done:\n",
        "                    players['X'].add_to_memory(current_state, action_x, next_state, reward, done)\n",
        "            else:\n",
        "                action_o = player.get_action(ttt)\n",
        "                next_state, reward, done, _ = ttt.step(action_o)\n",
        "\n",
        "                players['X'].add_to_memory(current_state, action_x, next_state, reward, done)\n",
        "\n",
        "    batch = random.sample(players[\"X\"].exp_replay, 128)\n",
        "\n",
        "    opt.zero_grad()\n",
        "    loss = compute_td_loss(batch, players[\"X\"])\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(players[\"X\"].Q_net.parameters(), 1.0)\n",
        "    opt.step()\n",
        "\n",
        "    if j % 50 == 0:\n",
        "        players[\"X\"].update_target_network()\n",
        "\n",
        "    if j % 500 == 0:\n",
        "        wr = get_winrate()\n",
        "\n",
        "    if wr > target_wr:\n",
        "        break\n",
        "        if players[\"X\"].epsilon > 0:\n",
        "            players[\"X\"].epsilon -= 0.005"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}